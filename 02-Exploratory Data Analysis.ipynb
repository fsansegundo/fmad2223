{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c678edc",
   "metadata": {},
   "source": [
    "## <span style='background:yellow; color:red'> Remember:</span>\n",
    "\n",
    "+ Navigate to your `fmad2223` folder in the console  \n",
    "+ Execute `git pull origin main` to update the code\n",
    "+ **Do not modify the files in that folder**, copy them elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26924db1",
   "metadata": {},
   "source": [
    "# Working with Data Files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd8ca8",
   "metadata": {},
   "source": [
    "## Basic Data Files (txt, csv) with Pandas.\n",
    "\n",
    "+ We will begin by downloading one of these files, called `movies.csv` containing information about the top watched films (in theaters) between 2007 and 2011. \n",
    "\n",
    "+ We strongly recommend that you always create a project specific `data` folder and store all the downloaded data files for each project in there. In our case you can keep this folder as a subfolder of the *Course Folder*. In particular, do not place the files inside the *Common Repository folder* (the one named *fmad2223*). Use the following link to download the data file (it is advised to use the right button of your mouse and choose an option such as \"download the linked file as...\"):  \n",
    "    [movies.csv](https://gist.githubusercontent.com/tiangechen/b68782efa49a16edaf07dc2cdaa855ea/raw/0c794a9717f18b094eabab2cd6a6b9a226903577/movies.csv)\n",
    "\n",
    "+ Before proceeding to read the file with Python **you should always begin examining the contents of a data file with a text editor** (such as Notepad++ in Windows or BBedit in Mac). Not doing so will often result in bad data processing downstream. And please refrain from using spreadsheets (such as Excel) for this exploratory tasks. Not only will it fail too often, in some cases it can lead to data loss.  \n",
    "\n",
    "+ In this case opening the file in a text editor (BBedit in the Mac here) shows something like this (only the first few lines are shown):\n",
    "![](fig/010-movies_csv_open_text_editor.png)\n",
    "There are some key ingredients of such files that you need to pay attention to here, even in simple cases like this first example:\n",
    "    + Each line of the file corresponds to a row of the data table. \n",
    "    + The first line or *header* is often a special line, containing the names of the table columns. In the best case scenario these correspond to the variables in our data.\n",
    "    + The individual column values in every line are separated by a fixed symbol: a comma, a semicolon, a space or tab, etc. This will be called the *separator* and it is critical to determine which one was used in each data file. For movies.csv the separator is a comma.\n",
    "    \n",
    "+ Now that we have some information about the file we are ready to load it into a Pandas Dataframe, that we will also call `movies`. Note that we first need to import Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('data/movies.csv', sep=\",\", header=0)\n",
    "\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268801e8",
   "metadata": {},
   "source": [
    "+ The arguments `sep` and `header` provide the Pandas [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function with the information we discovered in our initial file exploration. If you run the function like this:\n",
    "\n",
    "    ```python\n",
    "    movies = pd.read_csv('data/movies.csv')\n",
    "    ```\n",
    "    Pandas will try to *guess* which separator to use and whether or not there is a header line. This `movies.csv` file is such an easy example that the code will work either way. But be warned: this will sometimes fail and when working with many large files the guessing adds an unnecessary overhead to the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528bdea",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "+ using the commands we saw in `01a_test_your_setup` find out the shape (rows, columns) of this DataFrame.\n",
    "+ use the bracket selection (with `loc` or `iloc`) to get the `Genre` information for the movies from year 2010. What type of object did you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[movies.Year == 2010, 'Genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33d50d",
   "metadata": {},
   "source": [
    "### Reading the csv file from a URL (link)\n",
    "\n",
    "+ Sometimes it is simpler to skip the download part of the process and read the data directly from a link. Keep in mind, however, that for this to work the same caveats as before apply: it is better to explicitly set the separator, to know beforehand if there is a header in the file, etc.\n",
    "\n",
    "+ Let us see a classic example: the `mpg` data set can be downloaded [in csv format with this link](https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/mpg.csv). This data set contains information about fuel consumption (miles per gallon, thus mpg) and other characteristics of some car models from 1999 to 2008. You can read more aboout it and the variables it contains [in this link](https://ggplot2.tidyverse.org/reference/mpg.html).\n",
    "\n",
    "+ We use this example to show that we can use the link to the csv file as argument for Pandas `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48949b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_link = \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/mpg.csv\"\n",
    "mpg = pd.read_csv(mpg_link)\n",
    "\n",
    "print(mpg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19def427",
   "metadata": {},
   "source": [
    "### Creating and saving a csv file\n",
    "\n",
    "+ Next let us begin by creating a Pandas DataFrame with synthetic data. Learning to create such objects is an essential part of the data analysis skills required in this course. You will often need to simulate data with prescribed stratistical properties in order to perform computer experiments. These are in turn the key to understand the expected behavior in many real world problems. \n",
    "\n",
    "+ The NumPy and Scipy modules offer many tools for statistical modeling, and we will be meeting many of these tools in the next sessions. Here we begin with a simple example in which we use NumPy. You can [use this link](https://numpy.org/doc/1.16/reference/routines.random.html) to read the documentation for the functions we use and to get a first idea of the possibilities. For the time being it is ok if you don't fully understand some of the words below. We will discuss these issues later in the course:\n",
    "    + The `np.random.randint` function to generate 100 random integers between 1 and 20 (both included, of course with replacement). These become the Pandas Series `A`. Recall that each column of a Data.Frame is a Series.\n",
    "    + The `str.ascii_lowercase` string contains all the English alphabet lower case letters. We use a list comprehension to convert that to a NumPy array and then use `np.random.choice` to select 100 random letters with replacement. These form the Series `B`.\n",
    "    + Finally we use `np.random.random_sample` to obtain 100 random floating values uniformly distributed in the $[-4, 4]$ interval. These form the Series `C`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2022)  # seed for reproducibility\n",
    "\n",
    "# 100 random integers\n",
    "A = pd.Series(np.random.randint(low = 1, high = 7, size = 100))\n",
    "\n",
    "# 100 random lowercase letters\n",
    "import string as str\n",
    "alphabet_string = str.ascii_lowercase\n",
    "alphabet = np.array([alphabet_string[i] for i in range(len(alphabet_string))])\n",
    "B = pd.Series(np.random.choice(alphabet, size=100))\n",
    "\n",
    "# 100 random floats in [-4, 4]\n",
    "C = pd.Series(8 * np.random.random_sample(size=100) - 4)\n",
    "\n",
    "\n",
    "# Use the above as columns for a DataFrame\n",
    "X = pd.DataFrame({'A':A, 'B':B, 'C':C})\n",
    "print(X.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994423d",
   "metadata": {},
   "source": [
    "+ Now that we have created this DataFrame we can save it to a csv file. To identify it we will name that file `EDA_data_saving.csv` (in the `data` folder). Every Pandas DataFrame has a `to_csv` method that we can use for this. Run the following command and then use a text editor (Notepad++, BBEdit or similar) to check that the contents are what you expect. Make sure to check the header line, the separator and the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"data/EDA_data_saving.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be997c",
   "metadata": {},
   "source": [
    "+ **Exercise:** change the separator to a comma and set `index = True`. Then run the code and check the changes. \n",
    "\n",
    "    In general we recommend setting `index=False` unless you have a good reason not to. These makes reading the csv files in other programs easier. Be careful because the default behavior is `index=True`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1c503",
   "metadata": {},
   "source": [
    "## Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374b5c1",
   "metadata": {},
   "source": [
    "+ Excel files are frequently used as a data source in some contexts. For simple files, reading their content into a Pandas DataFrame is quite straightforward. But for more complex cases things can get more complicated. However, to keep things simple here we will use a simple file containing data about 2010 train accidents in the US. You can download the file (to the data folder) using [right-click in this link](http://users.stat.ufl.edu/~winner/data/train_acc_2010.xls). And the meaning of the variables in this dataset is [clarified by this link](https://users.stat.ufl.edu/~winner/data/train_acc_2010.txt)\n",
    "\n",
    "\n",
    "+ First we need to install an additional module into the fmad environment. This will often be the case when reading data from other sources. Do you remember how to install new modules?\n",
    "  **Exercise:** install the module called `xlrd` into the fmad environment.\n",
    "\n",
    "+ Then run this code to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85036b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = './data/train_acc_2010.xls'\n",
    "\n",
    "train_accidents = pd.read_excel(xlsx_file)\n",
    "\n",
    "print(train_accidents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de66a4",
   "metadata": {},
   "source": [
    "+ You can read the [documentation for the `read_excel` function here](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html). The examples therein will show you how to select a particular range of rows and columns in the spreadsheet and how to deal with the case when there are several sheets of data in the Excel file. And if you need to export directly from Python to Excel [read about the `to_excel` method here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9f2a8",
   "metadata": {},
   "source": [
    "# Different Types of Data\n",
    "\n",
    "+ The data sets we are beginning to see in these examples follow the two most basic rules of what is called *Tidy Data* (a term coined by Hadley Wickham in 2014):\n",
    "    + Each column of the table corresponds to a *variable*.\n",
    "    + Each row corresponds to an *observation*, *individual*, *case* or *sample*. All these terms refer to a single observational unit of whatever it is that we are considering.\n",
    "    \n",
    "+ The variables can be of different types: integer or floating point numbers, dates, binary variables (yes/no type), text strings, labels, dates, positions, etc. Those different types of variables allow for different operations and need specific treatment in many cases, if we are to get out of them as much information as possible. \n",
    "\n",
    "+ In the next sections we will meet the basic types of data (variables) and the best way to describe them. We will be using some of the examples that we have already seen and some others like the ones introduced here:\n",
    "\n",
    "    + The framingham data set contains health data from a heart disease study that took place in Framingham (UK). [This link contains the csv file URL](https://raw.githubusercontent.com/mbdfmad/fmad2122/main/data/framingham.csv) and you can [read more about the study and the variables in the table here](https://biolincc.nhlbi.nih.gov/media/teachingstudies/FHS_Teaching_Longitudinal_Data_Documentation.pdf?link_time=2019-08-26_14:42:24.487245).\n",
    "    + The `flights` data set contains information about flights that departed from New York in 2013. [This link contains the csv file URL](https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv). You can read more about this data set [in this link](https://rdrr.io/cran/nycflights13/man/flights.html).\n",
    "    \n",
    "+ **Exercise:** using the URLs above directly (without downloading) load these data sets into two Pandas DataFrames called, respectively, `framingham` and `flights`.   **Warning:** Some of the cells below will not work until you complete these exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2117b",
   "metadata": {
    "tags": [
     "raises-exception",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# %load \"code/EDA_exercise_import_csv.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb663f",
   "metadata": {
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "print(framingham.columns)\n",
    "print(framingham.head())\n",
    "print(framingham.describe())\n",
    "print(framingham.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b98993",
   "metadata": {
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "print(flights.columns)\n",
    "print(flights.head())\n",
    "print(flights.describe())\n",
    "print(flights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d9600",
   "metadata": {},
   "source": [
    "+ Let us pause here to think about the kind of data types in the examples we have seen. Many (but not all) of the variables / columns in the data tables fit into one of these categories:\n",
    "    + **Quantitative or Numeric Variables**. That means that the values of these variables are *numbers that have been measured in a meaningful scale, such that the operations with this numbers have meaning*. Numeric variables are further divided into **Discrete** and **Continuous** variables. We will discuss this with detail below.\n",
    "    + **Qualitative Variables or Factors**. The values of these variables are *labels* and they are used to *group the observations into classes*. A typical example would be grouping bird observations into species. The name of a species is a label, but it makes no sense to sum (or take the mean of) an ostrich and a sparrow. Even if the labels are sometimes numeric, operations have no sensible interpretation in relation with the data (we will see however an exception for binary variables when discussing proportions).  \n",
    "\n",
    "+ A numeric variable is **discrete** if there is a minimal or atomic unit of measurement such that all values of the variable are multiples of that unit. For example, when counting the coins in your wallet it makes no sense to say you have 3.5 coins. You may have 3 or 4, but no \"fraction of a coin\" counts as a coin.\n",
    "\n",
    "+ On the other hand a numeric variable is **continuous** if its values are measured in a floating point (decimal) scale, such that given two values then any intermediate values is, in principle, a possible value for the variable. \n",
    "\n",
    "+ We will see, however, that the division between discrete and continuous is not always as clear cut as it may appear at first. At the end of the day, it is usually a modeling choice. We will see many examples of why this happens. \n",
    "\n",
    "+ **Exercise / Discussion.** Look at the variables in some of the tables from the previous examples. How do they fit this types of variables? Are they quantitative or are they factors? In the case of numeric variables, do you consider them continuous or discrete?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecfbaa",
   "metadata": {},
   "source": [
    "# Discrete (Quantitative) Variables\n",
    "\n",
    "## Frequency Tables\n",
    "\n",
    "+ Let us look at the synthetic `X` DataFrame that we created before. The `A` variable is one example of a discrete numeric variable, as it only takes integer values (and few different values; more on this later). The first step in the exploration of a discrete variable is to obtain an **absolute frequency table**. For each value of the variable, this table counts the number of appearances of that value in the table. In Pandas you can get it using the `crosstab` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsFreq_X = X['A'].value_counts()\n",
    "AbsFreq_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b52c5",
   "metadata": {},
   "source": [
    "+ Note that the values in the table are not sorted by the size (they are jsut ordered by their order of appearance in the table). We will often want them ordered, especially in this case case of numeric variables. To get this ordered frequency table use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsFreq_X = X['A'].value_counts().sort_index()\n",
    "AbsFreq_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210bdd4",
   "metadata": {},
   "source": [
    "+ Absolute frequencies are often simply called *counts*. Of course the sum of the absolute frequencies equals the numer of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(AbsFreq_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d66e94",
   "metadata": {},
   "source": [
    "+ Absolute frequencies depend on the size of the whole data set. Sometimes, and specially when comparing two data sets, it is better to use *relative frequencies*. These are the answer to questions of like \"*What proportion of the values are equal to...?*\" In Pandas you can easily modify `value_counts` to get the relative frequencies or proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "RelFreq_X = X['A'].value_counts(normalize=True).sort_index()\n",
    "RelFreq_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a13309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(RelFreq_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1e289",
   "metadata": {},
   "source": [
    "### Some comments about relative frequencies\n",
    "\n",
    "+ Relative frequencies always add up to one. They are closely related to percents, but instead of using 100 as the total, we are using 1. \n",
    "+ More importantly, *relative frequencies are deeply connected with the first intuitive ideas about probability*. That is: imagine that you were to pick at random a row of the `X` table. What is the probability that the value of `A` in that row is 4. The relative frequency table tells us that we expect the value 4 to appear in 12 out of every 100 repetitions of that experiment.  \n",
    "\n",
    "### Cumulative frequencies (absolute and relative)\n",
    "\n",
    "+ Cumulative frequencies are used to respond to questions like \"how many values are lower than ...?\" os the analogous question in terms of proportions. We can easily obtain cumulative results using the `cumsum` method. Let us use it to get cumulative frequencies for `X`, both absolute and relative: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CumAbsFreq_X = AbsFreq_X.cumsum()\n",
    "CumAbsFreq_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da84bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CumRelFreq_X = RelFreq_X.cumsum()\n",
    "CumRelFreq_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02aa98e",
   "metadata": {},
   "source": [
    "+ The final cumulative frequency is always the total number of rows in the data set (in the absolute case) or 1 (in the relative case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040d10f",
   "metadata": {},
   "source": [
    "## Plots for discrete variables\n",
    "\n",
    "+ The basic and recommended graphical representation for a discrete variable is the *bar plot*. We will illustrate this with the `A` variable of `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = AbsFreq_X.plot.bar(rot=0, xlabel='A', ylabel ='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca3dfd",
   "metadata": {},
   "source": [
    "    The only difference when plotting the relative frequencies is the scale in the vertical axis.\n",
    "    \n",
    "+ The bar plot in this example gives us a visual summary of the *shape* of the data. In other words, we can see how the data is distributed among the different possible values. We will talk a lot about *distributions* in this course, and you can consider this humble bar plot as the first step in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['A'].value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbf934",
   "metadata": {},
   "source": [
    "# Continuous (Quantitative) Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd9e5e",
   "metadata": {},
   "source": [
    "+ We will first consider variables that are clearly treated as continuous in the data set. For example, the `C` variable in `X` was designed that way. For such a variable, frequency tables make no sense. Most values of such variables will appear only once in the table, making almost every absolute frequency equal to 1. We need a different set of tools for these variables.\n",
    "\n",
    "+ The simplest idea is to reduce the continuous variables to discrete ones using `binning`. That is we divide all  values of the variable into *bins*, which are a collection of intervals that span all possible values. In our present example, the variable `C` was designed to take random values in the $[-4, 4]$ interval. Thus we will divide that interval into a collection of subintervals or bins. How many bins? Well, that is not a simple question, because the answer depends on both the specific data and what you are trying to accomplish. A rule of thumb is not to use less than five bins and no more than $\\sqrt{n}$, where $n$ is the number of data points for your variable. In our case we will divide $[-4, 4]$ into eight subintervals or bins and we will assign each value of `C` to the corresponding subinterval. We can do that with the Pandas cut function. We will first apply it naïvely to the variable and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31231436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(X['C'], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5515be7",
   "metadata": {},
   "source": [
    "As you can see, the result is a Panda Series that contains the subinterval assigned to the corresponding values in `C`. But the endpoints of these intervals depend too much on the specifics of our data. Remember that these were randomly chosen in the $[-4, 4]$ interval. If we were to choose another sample of random points in the same interval, these endpoints probably wouldn't do. Thus we go back to `cut` but this time we explicitly set the endpoints of the bins (we call them *nodes* here), as a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_nodes = np.arange(-4, 5, 1)\n",
    "print(C_nodes)\n",
    "\n",
    "pd.cut(X['C'], bins=C_nodes, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506efff7",
   "metadata": {},
   "source": [
    "That looks much better. Note that we also took the chance to instruct Python to keep the subintervals ordered. In fact, the 'C_binned' variable is now an *ordered factor*, a concept that we will discuss below. We can now add that column to the DataFrame in a very simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C_binned'] = pd.cut(X['C'], bins=C_nodes, ordered=True)\n",
    "print(X.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872d347",
   "metadata": {},
   "source": [
    "This `C_binned` variable is a factor (a qualitative variable), because its values are not numbers, but labels (the names of the subintervals). We will discuss factors below, but for now suffice it to say that we can also use the concept of frequencies with factors. We get this table (again, the order is taken into account):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130621f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C_binned'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X['C_binned'].value_counts().sort_index().plot.bar(rot=0, xlabel='A', ylabel ='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daedf40",
   "metadata": {},
   "source": [
    "## Histograms and Density Plots\n",
    "\n",
    "\n",
    "The preceding plot is not bad, but it has some issues. The most relevant one is the fact that the *gaps* between the bars have lost utility. The division we made with cut created *consecutive intervals* qith no space between them. And this plot fails to convey that. The alternative is a classic type of plot for continuous data called the histogram. We can get it with Pandas method `hist`. Here we have tweaked the result a little to improve the resulting plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X.hist('C', edgecolor='black', linewidth=1.2, grid=False, color = \"tan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63648965",
   "metadata": {},
   "source": [
    "+ But you can notice here that `hist` has chosen the number of bins for us. It is often better to do this by hand. We wil use the `C_nodes` values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X.hist('C', edgecolor='black', linewidth=1.2, grid=False, color = \"tan\", bins=C_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ddb34",
   "metadata": {},
   "source": [
    "+ Please compare the histogram with the bar plot at the end of the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94b026",
   "metadata": {},
   "source": [
    "## Discrete - Continuous Transition\n",
    "+ Frequency tables become soon useless as the number of different values of a variable increases. For example, if we look at the `CarsDer` variable in the `train_accidents` DataFrame (indicates the number of derailed train cars in the accident) and we ask Python for a frequency table, we will get this answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accidents['CarsDer'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb28a3f",
   "metadata": {},
   "source": [
    "\n",
    "+ This is a common situation and it illustrates the kind of situation where the boundary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaeb009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4ae82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33060e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cca35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84a8f75f",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81b9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51fb6bd",
   "metadata": {},
   "source": [
    "# Measures of Center and Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc858f",
   "metadata": {},
   "source": [
    "# Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c1c0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff6b2d9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4a554",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f11484af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e6905f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c2cfcf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
